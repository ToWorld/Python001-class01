# 学习笔记
## homework one:
```启动方式: cd bs && python fetch_top10_film_info_from_maoyan.py```

## homework two:
```启动方式: cd scrapy/spiders/spiders && scrapy crawl movie```

## 待解决问题
### question one: 即使使用fake-user-agent以及使用cookies, 访问次数过多依旧会报403问题?
### question two: 通过pandas连续写csv, 只保证一个列名?

