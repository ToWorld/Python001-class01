# 本次功能更新:
```
1.使用随机UserAgent
2.使用代理IP(在github寻找免费代理IP)
```

# 目前爬虫还有几个待解决问题:
```
1.常见的验证码问题
2.从网页提取的数据有不符合预期的部分,因此还需要二次清洗
3.目前scrapy分布式中,每个爬虫的并发都是通过配置文件手动更新,需要一个轻松更改的API.
4.将多个爬虫的调度进行统一, 每个scrapy实例机器资源实现隔离
```
